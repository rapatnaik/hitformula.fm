<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Dashboard Write‑Up</title>
  <link rel="stylesheet" href="writeup.css" />
</head>
<body>
  <nav class="navbar">
    <a href="../index.html">Home</a>
    <a href="writeup.html">Write‑Up</a>
  </nav>

  <div class="container">
    <h1>Final Write‑Up</h1>

    <!-- 1. Motivation -->
    <section>
      <h2>1. Motivation & Compelling Question</h2>
      <p>
        Our dashboard answers the question:  
        <em>“How well do different facial regions (forehead, inner canthi, mouth) track true oral temperature under various ambient conditions and demographics?”</em>
      </p>
    </section>
    <section>
  <h2>2. Data Source & Subsetting</h2>
  <p>
    We utilized a publicly available dataset titled 
    <strong>“Facial and oral temperature data from a large set of human subject volunteers”</strong> from 
    <a href="https://physionet.org/content/face-oral-temp-data/1.0.0/" target="_blank">PhysioNet</a>, which provided a robust foundation for thermographic exploration.
  </p>

  <ul>
    <li><strong>Volunteers:</strong> Over 1,020 participants (IRT-1) and 1,009 participants (IRT-2)</li>
    <li><strong>Measurements:</strong> Facial and internal oral temperatures captured using infrared thermography (IRT)</li>
    <li><strong>Demographics:</strong> Gender, age, ethnicity, cosmetic use, and ambient room conditions</li>
    <li><strong>Regions recorded:</strong> Center/left/right forehead, inner eye corners, and mouth</li>
  </ul>

  <p>
    We narrowed our focus to facial thermographic data and grouped participants by room temperature:
  </p>
  <ul>
    <li><strong>Group 1:</strong> 20.0–24.0°C</li>
    <li><strong>Group 2:</strong> 24.0–28.0°C</li>
  </ul>
  <p>
    After filtering and cleaning the data, we created two finalized CSV files for Group 1 and Group 2 to support our real-time offset calculations across facial zones.
  </p>
</section>

    <section>
  <h2>3. Visual Encoding & Interaction Rationale</h2>

  <h3>Visual Encodings</h3>
  <ul>
    <li><strong>Color Gradient (Blue → Red):</strong> We used a perceptual color scale that mirrors how people already interpret temperature—cool blues for lower temps and hot reds for higher ones. This makes it easier to understand at a glance, without needing a legend.</li>
    <li><strong>Red Glow (≥ 38°C):</strong> We added a subtle red glow to any region that crossed the fever threshold. This acts as a visual alarm bell, helping users notice health concerns without digging into the data. It’s especially helpful when you’re comparing multiple facial regions quickly.</li>
    <li><strong>Circle Markers:</strong> Instead of shading the entire face, we placed interactive markers at key regions (forehead, eyes, mouth). These let users precisely select and simulate temperatures without cluttering the view—kind of like placing a thermometer in a spot you're curious about.</li>
  </ul>

  <h3>Interaction Techniques</h3>
  <ul>
    <li><strong>Temperature Slider (Simulation Tool)</strong> Clicking a region activates a thermometer slider, which simulates a measured temperature. Behind the scenes, we use real-world offset data to calculate how that temp would influence other regions. You instantly see the ripple effect—just like how a fever might affect your whole face differently.</li>
    <li><strong>Medical Alert Toggle (Fever Threshold)</strong> When “Show Medical Alerts” is turned on, we automatically flag any region with a temperature of 38°C or higher. It’s a helpful tool for screening: users don’t have to guess what’s normal or dangerous—we surface that visually.</li>
    <li><strong>Demographic Filters (Age + Gender):</strong> You can change the age range or gender, and the dashboard recalculates based on matching data. This allows users to see how facial heat patterns differ—for example, younger users may show different distributions than older ones under the same ambient conditions.</li>
    <li><strong>Real-Time Tooltips:</strong> Just hover over any dot, and we’ll show you the exact temperature reading. It’s a simple but powerful addition that brings clarity and precision to the experience.</li>
    <li><strong>Auto-Diagnosis Panel:</strong> Once you set a temperature, our tool analyzes the entire face and provides feedback. It doesn't just highlight regions—it explains what elevated temperatures might mean (e.g., “Possible fever,” “Sinus congestion”), and what to do next (e.g., hydrate, rest, or see a doctor). It’s like a mini health assistant powered by data.</li>
    <li><strong>Face Image Switching (By Gender):</strong> Based on the selected gender, the dashboard updates to show a male or female facial diagram. This helps the interface feel more personalized and reinforces the demographic filtering visually.</li>

  </ul>

  <h3>Alternatives Considered</h3>
  <p>
    We initially considered a heatmap-style grid showing all sensor data, including internal oral temperatures. While informative, it lacked clarity and visual engagement. Our current face-based interactive view made insights more tangible and accessible—especially for non-technical audiences. 
    We also discussed animated thermal progression, but prioritized clarity and performance.
  </p>
</section>

    <section>
      <h2>4. Team & Development Process</h2>
      

        <li>
          Our team worked closely together during the development process. After spending some time brainstorming and discussing the outline of the final 
          visualization and the features it would have, we agreed on a structure and how to split the tasks
          </li>

        <li><strong>Manasa:</strong> Led the dashboard design, integrated facial overlays, developed the auto-diagnosis logic, and handled interface styling.
</li>
        <li><strong>Rakshan:</strong> Focused on D3 integration, preprocessing the dataset, and computing temperature offsets dynamically based on selected facial zones.
</li>
        <li><strong>Andrew:</strong> Optimized layout responsiveness, implemented the write-up section, and ensured clean visual design across both desktop and mobile views.</li>
        <li>Despite split responsibilities, our team continually maintained consistent communication and meetings to discuss progress, ask questions, 
          and suggest features that we could add. This allowed our team to follow our timeline in an efficient and creative manner.
        </li>

        <strong>What we learnt:</strong> One of the biggest things we learned while working on this project was just how messy real-world data can be. When we first got the dataset, we thought we could just plug it into D3 and start visualizing, but it took a lot of time to clean, merge, and figure out which columns were actually useful. Getting the offsets between facial regions to work correctly also took more effort than expected, especially since we had to think about different ambient temperature groups and how to show realistic values when users moved the temperature slider. On the design side, we realized how important it is to keep things simple. We started with some complex ideas like heatmaps and time-based animations, but ended up building an interactive face because it was way more intuitive and visually engaging. Another technical challenge was making sure the facial markers aligned correctly on both male and female templates because just changing the image meant we had to completely rethink the coordinates. Finally, adding the Auto Diagnosis box helped us practice turning raw data into something people could actually understand and act on. Overall, we learned how to connect data science, design, and user experience all in one project, and it made us think way more about how people actually use visualizations.</li>
   
        </li>


      <h3>Workflow & Time</h3>

        <li><strong>Total effort:</strong> ~30 people‑hours</li>
        <li>To work on the project, we had in person and online meetings while having a group
           chat on the side to post questions and updates.</li>
        <li><strong>Challenges:</strong> One of the most technically complex challenges we tackled was designing the temperature offset model across different facial regions. It was not just about plotting raw temperature readings, we had to account for how each region, such as the forehead, cheeks, and nose, naturally varies in baseline temperature. That meant calculating relative offsets rather than relying on absolute values. The tricky part was making these offsets dynamic in D3. We needed to ensure that if one region's temperature changed, the related regions would update accordingly in real time. This required a lot of iteration with both the mathematical logic behind the offsets and D3’s data-binding system, but it helped us deepen our understanding of how to build responsive and interactive visualizations.









        </li>

    </section>
  </div>
</body>
</html>
